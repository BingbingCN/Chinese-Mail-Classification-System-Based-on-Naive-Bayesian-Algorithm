{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 项目流程\n",
    "\n",
    "任务理解\n",
    "\n",
    "- 是分类任务还是回归任务\n",
    "\n",
    "数据准备\n",
    "\n",
    "- 什么类型的数据（文本、图像）\n",
    "- 数据的标签是是什么样子的\n",
    "- 数据清洗\n",
    "\n",
    "模型\n",
    "- 机器学习模型（朴素贝叶斯、SVM）\n",
    "- 深度学习模型（几层神经网络、预训练模型有可以借鉴的吗？）\n",
    "\n",
    "模型训练与测试\n",
    "\n",
    "模型部署\n",
    "- 网页部署 Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务理解\n",
    "识别垃圾短信，有两种情况，是垃圾短信和不是垃圾短信，是与不是的问题，是一个分类问题，用分类的思路解决此任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备\n",
    "\n",
    "第一步就是查看原始数据，展示如下：\n",
    "\n",
    "```\n",
    "1\t尊敬的贵宾您好！金桥足道x月x日正常营业！xxxx年与邢博士强强联手！更新大批技师团队！形象好，气质佳，手法一流，包您满意！电话xxx\n",
    "0\t希望你们能懂我视你们如珍宝不容被欺不容被辱世间的不公对你们恶意相向而你们终将会如同凤凰一般洗尽污秽涅槃重生闪耀于这世间\n",
    "```\n",
    "第一列是标签，1 表示 垃圾短信，0 表示 正常短信\n",
    "\n",
    "第二列是短信文本，一般可以用文本处理的工具，例如 jieba 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba 分词 处理文本\n",
    "\n",
    "把一句话变成一个又一个词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m:  ['1', '南口阿玛施新春第一批限量春装到店啦\\ue310   \\ue310   \\ue310   春暖花开淑女裙、冰蓝色公主衫\\ue006   气质粉小西装、冰丝女王长半裙、\\ue319   皇\\n']\n",
      "\n",
      "\n",
      "flag:  1\n",
      "\n",
      "\n",
      "messList:  ['南口', '阿玛施', '新春', '第一批', '限量', '春装', '到', '店', '啦', '\\ue310', ' ', ' ', ' ', '\\ue310', ' ', ' ', ' ', '\\ue310', ' ', ' ', ' ', '春暖花开', '淑女', '裙', '、', '冰', '蓝色', '公主', '衫', '\\ue006', ' ', ' ', ' ', '气质', '粉小', '西装', '、', '冰丝', '女王', '长半裙', '、', '\\ue319', ' ', ' ', ' ', '皇', '\\n']\n",
      "\n",
      "\n",
      "m:  ['0', '带给我们大常州一场壮观的视觉盛宴\\n']\n",
      "\n",
      "\n",
      "flag:  0\n",
      "\n",
      "\n",
      "messList:  ['带给', '我们', '大', '常州', '一场', '壮观', '的', '视觉', '盛宴', '\\n']\n",
      "\n",
      "\n",
      "m:  ['0', '感到自减肥、跳减肥健美操、\\n']\n",
      "\n",
      "\n",
      "flag:  0\n",
      "\n",
      "\n",
      "messList:  ['感到', '自', '减肥', '、', '跳', '减肥', '健美操', '、', '\\n']\n",
      "\n",
      "\n",
      "m:  ['1', '感谢致电杭州萧山全金釜韩国烧烤店，本店位于金城路xxx号。韩式烧烤等，价格实惠、欢迎惠顾【全金釜韩国烧烤店】\\n']\n",
      "\n",
      "\n",
      "flag:  1\n",
      "\n",
      "\n",
      "messList:  ['感谢', '致电', '杭州', '萧山', '全金', '釜', '韩国', '烧烤店', '，', '本店', '位于', '金城', '路', 'xxx', '号', '。', '韩式', '烧烤', '等', '，', '价格', '实惠', '、', '欢迎惠顾', '【', '全金', '釜', '韩国', '烧烤店', '】', '\\n']\n",
      "\n",
      "\n",
      "m:  ['1', '一次价值xxx元王牌项目；可充值xxx元店内项目卡一张；可以参与V动好生活百分百抽奖机会一次！预约电话：xxxxxxxxxxx\\n']\n",
      "\n",
      "\n",
      "flag:  1\n",
      "\n",
      "\n",
      "messList:  ['一次', '价值', 'xxx', '元', '王牌', '项目', '；', '可', '充值', 'xxx', '元店', '内', '项目', '卡', '一张', '；', '可以', '参与', 'V', '动好', '生活', '百分百', '抽奖', '机会', '一次', '！', '预约', '电话', '：', 'xxxxxxxxxxx', '\\n']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "testDataNum = 5 \n",
    "path_to_testMess = '/Users/apple/Desktop/整理/计算机视觉/代码/MailClassify/data/filterMess.txt'\n",
    "testPostingList = [] # 收集文本\n",
    "testClassVec  = [] # 收集标签\n",
    "\n",
    "\n",
    "f = open(path_to_testMess,'r')\n",
    "n = 0\n",
    "\n",
    "while n < testDataNum:\n",
    "\n",
    "    n = n + 1\n",
    "    line = f.readline()\n",
    "    m = re.split('\\t',line) \n",
    "    print('m: ', m)\n",
    "    print('\\n')\n",
    "    flag = int(m[0])  # str -> int 标签\n",
    "    print('flag: ', flag)\n",
    "    print('\\n')\n",
    "    mess = m[1]\n",
    "    messList = jieba.lcut(mess, cut_all=False) \n",
    "    print('messList: ', messList)\n",
    "    print('\\n')\n",
    "    testPostingList.append(messList)\n",
    "    testClassVec.append(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testPostingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['南口',\n",
       "  '阿玛施',\n",
       "  '新春',\n",
       "  '第一批',\n",
       "  '限量',\n",
       "  '春装',\n",
       "  '到',\n",
       "  '店',\n",
       "  '啦',\n",
       "  '\\ue310',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  '\\ue310',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  '\\ue310',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  '春暖花开',\n",
       "  '淑女',\n",
       "  '裙',\n",
       "  '、',\n",
       "  '冰',\n",
       "  '蓝色',\n",
       "  '公主',\n",
       "  '衫',\n",
       "  '\\ue006',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  '气质',\n",
       "  '粉小',\n",
       "  '西装',\n",
       "  '、',\n",
       "  '冰丝',\n",
       "  '女王',\n",
       "  '长半裙',\n",
       "  '、',\n",
       "  '\\ue319',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  '皇',\n",
       "  '\\n'],\n",
       " ['带给', '我们', '大', '常州', '一场', '壮观', '的', '视觉', '盛宴', '\\n'],\n",
       " ['感到', '自', '减肥', '、', '跳', '减肥', '健美操', '、', '\\n'],\n",
       " ['感谢',\n",
       "  '致电',\n",
       "  '杭州',\n",
       "  '萧山',\n",
       "  '全金',\n",
       "  '釜',\n",
       "  '韩国',\n",
       "  '烧烤店',\n",
       "  '，',\n",
       "  '本店',\n",
       "  '位于',\n",
       "  '金城',\n",
       "  '路',\n",
       "  'xxx',\n",
       "  '号',\n",
       "  '。',\n",
       "  '韩式',\n",
       "  '烧烤',\n",
       "  '等',\n",
       "  '，',\n",
       "  '价格',\n",
       "  '实惠',\n",
       "  '、',\n",
       "  '欢迎惠顾',\n",
       "  '【',\n",
       "  '全金',\n",
       "  '釜',\n",
       "  '韩国',\n",
       "  '烧烤店',\n",
       "  '】',\n",
       "  '\\n'],\n",
       " ['一次',\n",
       "  '价值',\n",
       "  'xxx',\n",
       "  '元',\n",
       "  '王牌',\n",
       "  '项目',\n",
       "  '；',\n",
       "  '可',\n",
       "  '充值',\n",
       "  'xxx',\n",
       "  '元店',\n",
       "  '内',\n",
       "  '项目',\n",
       "  '卡',\n",
       "  '一张',\n",
       "  '；',\n",
       "  '可以',\n",
       "  '参与',\n",
       "  'V',\n",
       "  '动好',\n",
       "  '生活',\n",
       "  '百分百',\n",
       "  '抽奖',\n",
       "  '机会',\n",
       "  '一次',\n",
       "  '！',\n",
       "  '预约',\n",
       "  '电话',\n",
       "  '：',\n",
       "  'xxxxxxxxxxx',\n",
       "  '\\n']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testPostingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testClassVec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testClassVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本表示\n",
    "计算机存储文本或是图像，本质是存储一堆数字，因为要把文本表示为数字\n",
    "\n",
    "###  词的 one-hot 表示\n",
    "\n",
    "假设我们有一个词典库【“我们”，“去”，“爬山”，“今天”，“你们”，“昨天”，“跑步”】，\n",
    "\n",
    ">对于 “我们” 这个词，可以用这么一个向量表示，向量的维度是词典库的长度，这个向量的元素是 0 和 1。\n",
    ">\n",
    ">由于“我们”在词典中是第一个位置，所以对应的向量第一个元素是 1，其他位置元素是 0，即【1,0,0,0,0,0,0】\n",
    ">\n",
    ">对于 “去” 这个单词，它在词典库中是第二个位置，所以对应的向量第二个元素是1，其他元素是0，即【0,1,0,0,0,0,0】\n",
    ">\n",
    ">同理，对于“你们”这个单词，对应的 one-hot 向量为【0,0,0,0,1,0,0】\n",
    "\n",
    "\n",
    "### 句子的 one-hot 表示\n",
    "\n",
    "例如，对于 “我们今天去爬山” 这一句话，仍然构造一个向量，且向量的维度等于词典的大小。这个向量的元素也是0 或者 1。如果某个单词在句子中出现了，那么向量中对应位置的元素是 1，否则是 0。\n",
    "\n",
    "> 例如，“我们” 在词典中位置是 1，所以向量第一个元素是 1，\n",
    ">\n",
    "> “今天” 在词典中的位置是 4，所以向量的第 4 个元素是 1，\n",
    ">\n",
    ">“去” 在词典中的位置是 2，所以向量的第 2 个元素是 1，\n",
    ">\n",
    ">“爬山” 在词典中的位置是 3，所以向量的第 3 个元素是 1。\n",
    ">\n",
    ">而向量的其他位置的元素则是0。\n",
    ">\n",
    ">该向量为【1,1,1,1,0,0,0】\n",
    ">\n",
    "> 而对于“你们昨天去爬山”这一句话，因为“你们”在词典库中的第5个位置，“昨天”在第6个位置，所以对应的one-hot向量是【0,1,1,0,1,1,0】\n",
    "\n",
    "总结：句子对应的 one-hot 向量维度也是词典库的长度，元素是 0 或者 1。对于向量的第 k 个元素，如果词典库中的第 k 个词出现在句子中，那么其值为 1，否则为 0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本次任务该如何处理\n",
    "\n",
    "- 想象有个字典，统计训练集中出现的所有词，变成一个小字典\n",
    "- 设置一个向量，向量的长度与小字典的长度一样长\n",
    "- 一个短信出现了哪个词，这个词在向量上的位置记录为 1，否则为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vocabSet : {'啦', '裙', '、', '气质', '蓝色', '长半裙', '到', '粉小', '新春', '公主', '西装', '\\ue319', '皇', '\\ue006', '衫', '限量', '淑女', '店', '女王', '冰丝', '\\n', ' ', '南口', '春装', '春暖花开', '\\ue310', '冰', '阿玛施', '第一批'}\n",
      "\n",
      "\n",
      " vocabSet : {'啦', '裙', '一场', '常州', '、', '气质', '蓝色', '盛宴', '长半裙', '的', '壮观', '到', '粉小', '新春', '公主', '西装', '\\ue319', '皇', '\\ue006', '衫', '视觉', '限量', '淑女', '店', '女王', '大', '冰丝', '我们', '\\n', ' ', '南口', '带给', '春装', '春暖花开', '\\ue310', '冰', '阿玛施', '第一批'}\n",
      "\n",
      "\n",
      " vocabSet : {'啦', '裙', '常州', '感到', '、', '减肥', '蓝色', '盛宴', '长半裙', '的', '自', '健美操', '到', '粉小', '新春', '西装', '\\ue319', '\\ue006', '视觉', '限量', '冰丝', '\\n', ' ', '春暖花开', '\\ue310', '冰', '阿玛施', '一场', '气质', '跳', '壮观', '公主', '皇', '衫', '淑女', '店', '女王', '大', '南口', '带给', '春装', '我们', '第一批'}\n",
      "\n",
      "\n",
      " vocabSet : {'啦', '裙', '常州', '全金', '感到', '、', '减肥', '蓝色', '盛宴', '长半裙', '的', '釜', '自', '欢迎惠顾', '韩国', '健美操', '杭州', '到', '粉小', '新春', '西装', '\\ue319', '\\ue006', '视觉', '限量', '冰丝', '\\n', ' ', '春暖花开', '\\ue310', '冰', '阿玛施', '实惠', '感谢', '烧烤店', '一场', '位于', 'xxx', '气质', '萧山', '跳', '壮观', '金城', '韩式', '【', '，', '路', '号', '致电', '公主', '皇', '本店', '衫', '淑女', '店', '女王', '大', '南口', '】', '价格', '带给', '烧烤', '春装', '等', '我们', '第一批', '。'}\n",
      "\n",
      "\n",
      " vocabSet : {'裙', '全金', '内', '感到', '、', '减肥', '蓝色', '元店', '盛宴', '机会', '的', '充值', '欢迎惠顾', '百分百', '卡', '杭州', '粉小', '一张', '西装', '\\ue006', '动好', '限量', '冰丝', '参与', ' ', '抽奖', '电话', '可以', '春暖花开', '\\ue310', '预约', '阿玛施', '实惠', '感谢', '一场', '项目', '：', '位于', '气质', '跳', '壮观', '金城', '【', '元', '路', '一次', '公主', '本店', '衫', '；', '店', '女王', '大', '南口', '】', '价格', '带给', '烧烤', '春装', '价值', '我们', '第一批', '。', 'xxxxxxxxxxx', '啦', '常州', '长半裙', '釜', '生活', '自', '韩国', '健美操', '王牌', 'V', '到', '新春', '\\ue319', '视觉', '！', '\\n', '冰', '烧烤店', 'xxx', '萧山', '，', '号', '致电', '皇', '淑女', '可', '等', '韩式'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "vocabSet = set([])\n",
    "n = 0\n",
    "for doc in testPostingList:\n",
    "    n += 1\n",
    "    # 并集，取两集合全部的元素\n",
    "    vocabSet = vocabSet | set(doc)\n",
    "    print(' vocabSet :', vocabSet)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabList = list(vocabSet)\n",
    "returnVec = [0]*len(vocabList) #与词汇表等长的list\n",
    "\n",
    "inputSet = testPostingList[1]\n",
    "\n",
    "for word in inputSet:\n",
    "    if word in vocabList:\n",
    "        returnVec[vocabList.index(word)] += 1\n",
    "        # returnVec从0开始\n",
    "    #else:\n",
    "    #\tprint \"the word: %s is not in my Vocabulary!\" % word\n",
    "\n",
    "returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原始代码之导入数据-在此 notebook 上运行-还需调整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    try:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "# 一般来说数据是全部导入，有时候设备没有那么强大时，可选部分数据\n",
    "trainDataNum = 8000  # 设置训练数据的数量，10w 条时 8G 内存不够\n",
    "testDataNum = 5000  # 设置测试数据的数量\n",
    "\n",
    "# 训练集和测试集的地址，一般要替换成当前设备下的地址\n",
    "path_to_testMess = '/Users/apple/Desktop/整理/计算机视觉/代码/MailClassify/data/filterMess.txt'\n",
    "# 28.8M 一般测试文件小，训练文件大\n",
    "path_to_trainMess = '/Users/apple/Desktop/整理/计算机视觉/代码/MailClassify/data/带标签短信.txt'\n",
    "# 58.2M\n",
    "\n",
    "\n",
    "'''\n",
    "testLoadData(): 加载test数据，已除去train数据\n",
    "\n",
    "# testPostingList:数据为list的list，把垃圾短信进行了分词\n",
    "[['我','最近','在','学车'],['然后','科二','科三','都','挂了'],['是的','都','挂了']]\n",
    "\n",
    "# testClassVec:一维标签 list，表示 testPostingList 中数据的类型\n",
    "[1,1,0,0,1,0]\n",
    "testClassVec length:  5000\n",
    "testPostingList length:  5000\n",
    "vocabSet length:  19631\n",
    "\n",
    "'''\n",
    "\n",
    "def testLoadData():\n",
    "\ttestPostingList = []; testClassVec = []\n",
    "    try:\n",
    "        f = open(path_to_testMess,'r')\n",
    "        n = 0\n",
    "\t\tif not f:\n",
    "\t\t\tprint('open file failed~')\n",
    "\t\t\traise ValueError('open file failed~')\n",
    "\t\tprint('open file succeed~')\n",
    "        \n",
    "\t\twhile n < trainDataNum:  \n",
    "\t\t\tn = n + 1\n",
    "\t\t\tline = f.readline()\n",
    "\t\t\n",
    "        n = 0\n",
    "\t\twhile n < testDataNum:\n",
    "\t\t\tn = n + 1\n",
    "\t\t\tline = f.readline()\n",
    "\t\t\tm = re.split('\\t',line)  # 返回list\n",
    "\t\t\t# m:  ['0', '飞机机身上喷的是“淘宝网”字样\\n']\n",
    "\t\t\t# print('m:', m) #显示编码\n",
    "\t\t\tflag = int(m[0])  # str -> int 标签\n",
    "\t\t\tmess = m[1]  # str类型 短信内容 \n",
    "\t\t\tmessList = jieba.lcut(mess,cut_all=False) # 短信分词\n",
    "\t\t\t# messlist:  ['飞机', '机身', '上', '喷', '的', '是', '“', '淘宝网', '”', '字样', '\\n']\n",
    "\t\t\t# jieba.lcut 直接返回list\t\n",
    "\t\t\tif(n % 1000 == 0):\n",
    "\t\t\t\tprint('testLoadData：',n)\n",
    "\t\t\t\n",
    "\t\t\t# print('messlist: ', messList) # 一条短信分词的list,没处理\n",
    "\t\t\ttestPostingList.append(messList) # 收集所有短信内容\n",
    "\t\t\ttestClassVec.append(flag) # 收集所有短信的标签\n",
    "\t\t# print('testClassVec length: ', len(testClassVec))\n",
    "\t\t# print('testPostingList length: ', len(testPostingList)) #print postingList\n",
    "\tfinally:\n",
    "\t\tif f:\n",
    "\t\t\tf.close()\n",
    "\treturn testPostingList, testClassVec\n",
    "\n",
    "\n",
    "'''\n",
    "LoadData(): 加载train数据\n",
    "\n",
    "# PostingList:数据为list的list，把垃圾短信进行了分词\n",
    "[['我','最近','在','学车'],['然后','科二','科三','都','挂了'],['是的','都','挂了']]\n",
    "\n",
    "# ClassVec:一维标签list，表示testPostingList中数据的类型\n",
    "[1,1,0,0,1,0]\n",
    "\n",
    "classVec length:  8000\n",
    "postingList length:  8000\n",
    "vocabSet length:  24552\n",
    "\n",
    "'''\n",
    "\n",
    "def loadData():\n",
    "\n",
    "\tpostingList = []; classVec = []\n",
    "\ttry:\n",
    "\t\tf = open(path_to_trainMess,'r', encoding='utf-8')\n",
    "\t\tn = 0\n",
    "\t\tif f:\n",
    "\t\t\tprint('open file succeeed')\n",
    "\t\twhile n < trainDataNum:  \n",
    "\t\t\tn = n + 1\n",
    "\t\t\tline = f.readline()\n",
    "\t\t\tm = re.split('\\t',line)  # 返回list\n",
    "\t\t\t# print('m: ', m) # 显示编码\n",
    "\t\t\tflag = int(m[0])  # str -> int\n",
    "\t\t\tmess = m[1]  #str类型\n",
    "\t\t\tmessList = jieba.lcut(mess,cut_all=False)\n",
    "\t\t\tif(n % 1000 == 0):\n",
    "\t\t\t\tprint('loadData',n)\n",
    "\t\t\t# print(str(messList).decode('string_escape'))\n",
    "\t\t\t# print('messlist: ', messList) # 一条短信分词的list, 没处理\n",
    "\t\t\tpostingList.append(messList)\n",
    "\t\t\tclassVec.append(flag)\n",
    "\t\t# print('classVec length: ', len(classVec))\n",
    "\t\t# print('postingList length: ', len(postingList)) #print postingList\n",
    "\tfinally:\n",
    "\t\tif f:\n",
    "\t\t\tf.close()\n",
    "\treturn postingList, classVec\n",
    "\n",
    "'''\n",
    "createVocabList(dataSet):构建词向量列表\n",
    "\n",
    "# dataSet:loadData() 函数返回的 postingList\n",
    "[['我','最近','在','学车'],['然后','科二','科三','都','挂了'],['是的','都','挂了']]\n",
    "\n",
    "# vocabSet:一维list，表示PostingList中所有词的集合\n",
    "['我','最近','在','学车','然后','科二','科三','都','挂了','是的']\n",
    "\n",
    "'''\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "\tvocabSet = set([])\n",
    "\tn = 0\n",
    "\tfor doc in dataSet:\n",
    "\t\tn += 1\n",
    "\t\t# 并集，取两集合全部的元素\n",
    "\t\tvocabSet = vocabSet | set(doc)\n",
    "\t\tif n % 1000 == 0:\n",
    "\t\t\tprint('createVocabList',n)\n",
    "\treturn list(vocabSet)\n",
    "\n",
    "'''\n",
    "word2Vec(vocabList, inputSet): 把分词后的短信转化为词向量\n",
    "\n",
    "#vocabList:createVocabList函数返回的vocabSet\n",
    "vocabSet:一维list，['我','最近','在','学车','然后','科二','科三','都','挂了','是的']\n",
    "\n",
    "#inputSet:分词后的短信list。可以是postingList中的一个元素。\n",
    "['我','最近','在','学车']\n",
    "\n",
    "returnVec:一维list，为词向量。\n",
    "[1,1,0,0,1,0]\n",
    "'''\n",
    "\n",
    "def word2Vec(vocabList, inputSet):\n",
    "\treturnVec = [0]*len(vocabList) #与词汇表等长的list\n",
    "\tfor word in inputSet:\n",
    "\t\tif word in vocabList:\n",
    "\t\t\treturnVec[vocabList.index(word)] += 1\n",
    "\t\t\t# returnVec从0开始\n",
    "\t\t#else:\n",
    "\t\t#\tprint \"the word: %s is not in my Vocabulary!\" % word\n",
    "\treturn returnVec  #list 类型\n",
    "\n",
    "##### test code ##### \n",
    "# postingList, classVec = loadData() \n",
    "# vocabSet = createVocabList(postingList) # vocabSet是词汇表\n",
    "# print('vocabSet length: ', len(vocabSet))\n",
    "\n",
    "# for doc in postingList:\n",
    "# \tprint(word2Vec(vocabSet, doc),'ooc') #某条短信对应的词向量\n",
    "# \t#returnVec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型\n",
    "\n",
    "朴素贝叶斯\n",
    "\n",
    "- 理解原理\n",
    "- 理解数学\n",
    "- 数学与代码的对应\n",
    "\n",
    "## 理解原理与数学\n",
    "\n",
    "**接下来以垃圾邮件为例来说明，邮件和短信类似，可以无障碍迁移。**\n",
    "\n",
    "朴素贝叶斯是我们的一个经典的算法模型，朴素贝叶斯特别适合用于文本的分类任务，在数据量没有那么多或者数据量比较多的时候它的效果都还是有很大的优势的，朴素贝叶斯最经典的场景就是做垃圾邮件的分类器。\n",
    "\n",
    "比如说我们打开了我们的邮箱，一些邮件是会自动被分类到垃圾邮件里面的，所以偶尔我们会去垃圾邮件里面找一些有用的邮件，但是绝大部分都是分类正确的，这样的邮箱分类器怎么做呢？我们通过朴素贝叶斯的方法可以非常快速的实现这样一个这种分类器。\n",
    "\n",
    "另外比如我要去分类一个文章，它是属于体育类的还是娱乐类的，我们仍然可以采用朴素贝叶斯的方法去实现。\n",
    "\n",
    "![](./img/1.jpg)\n",
    "\n",
    "如上图所示，就是现在我们想分类一个邮件为正常邮件还是垃圾邮件，这是一个最经典的二分类问题，在给定了一批正常邮件和垃圾邮件之后，再给一个新的邮件的时候，我希望我的算法可以把它正确分类成正常邮件或者垃圾邮件。\n",
    "\n",
    "- img 文件夹\n",
    "- .ipynb 文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯如何识别垃圾邮件\n",
    "\n",
    "朴素贝叶斯会不断的去统计正常邮件和垃圾邮件里面出现了什么样的单词，这也比较符合我们的常理，我们人去判断垃圾邮件还是正常邮件的时候，也是通过这种思路去判断的，比如一个邮件里面出现了莫名其妙的链接，那这个邮件就很有可能是垃圾邮件的，或者一个邮件里面经常出现像购买或者产品这些关键词的话，我们很大概率上也可以认为这个邮件可能是垃圾邮件的。\n",
    "\n",
    "![](./img/2.jpg)\n",
    "\n",
    "朴素贝叶斯的一个核心思想就是不断的去统计单词或者词语在正常邮件和垃圾邮件里面出现的概率是怎样的，一些敏感的词如广告、购买等等在垃圾邮件里面是肯定会经常出现的，通过这个方法我可以预测出未来一个新的邮件来了，这个邮件里面有没有包含这些敏感词，如果包含了很多，那我可以认为这是封垃圾邮件。\n",
    " \n",
    "所以我们在去构建这种朴素贝叶斯模型的时候相当于我要做很多方面的统计，比如一个单词在垃圾邮件里面出现的概率是多少，然后在正常邮件里面出现的概率是多少，我需要把每一个这种概率统计出来，统计完之后，我们在对新邮件做预测的时候，我们就可以把这些概率用起来，然后通过一些简单的操作，就可以做一个分类的决策。\n",
    "\n",
    "![](./img/3.jpg)\n",
    "\n",
    "如果现在我们要预测邮件内容是“购买物品，不是广告”这一封邮件是不是垃圾邮件该怎么预测呢？我们首先就要计算邮件中每一个单词在垃圾邮件里面出现的概率和在正常邮件里面出现的概率是多少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算单词的条件概率\n",
    "\n",
    "![](./img/4.jpg)\n",
    "\n",
    "我们给定了一批训练数据，如上图我们有24个正常邮件，有12个垃圾邮件，然后做了一个假设，每个邮件里面包含10个单词，所以正常邮件总共包含了240个单词，垃圾邮件总共包含了120个单词。\n",
    "\n",
    "接下来的主要工作就是要做统计，购买这个词是一个敏感词，从直观的角度来看，购买这个词在垃圾邮件里面出现的次数更多，基于这样的一个思路，现在要统计每一个单词在不同分类里面出现的概率是多少？\n",
    "\n",
    "![](./img/5.jpg)\n",
    "\n",
    "首先计算在正常邮件里面出现“购买”这个词的概率是多少，如图所示，“购买”在正常邮件里面出现了3次，正常邮件总共有240个单词，所以P(购买|正常邮件)= 3/240 = 1/80，所以正常邮件含有“购买”词的概率是 1/80.\n",
    "\n",
    "接着计算“购买”在垃圾邮件里面出现的概率，“购买”在垃圾邮件里面一共出现了7次，垃圾邮件总共有120个单词，所以P(购买|垃圾邮件)=7/120， P(购买|正常邮件) 明显小于P(购买|垃圾邮件)\n",
    " \n",
    " ![](./img/6.jpg)\n",
    " \n",
    "接下来我们再看另外一个关键词叫做“物品”，类似于购买，我们也统计一下出现“物品”的概率，“物品”出现在正常邮件的概率为 P(物品|正常邮件)=4/240=1/60，“物品”在垃圾邮件里面出现的概率为P(物品|垃圾邮件)=4/120=1/30，物品在垃圾邮件里面出现的概率是出现在正常邮件的2倍，所以一个邮件出现了物品，我就很有可能认定为这是一个垃圾邮件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![](./img/7.jpg)\n",
    " \n",
    " 然后我们计算“不是”这个单词，“不是”在正常邮件里面出现的概率 $P(不是|正常邮件)=4/240=1/60$，“不是”在垃圾邮件里面出现的概率 $P(不是|垃圾邮件)=3/120=1/40$，表面这个单词更倾向于在垃圾邮件里面出现的次数多。\n",
    " \n",
    "  ![](./img/8.jpg)\n",
    "  \n",
    " 最后计算“广告”这个单词，“广告”在正常邮件里面出现的概率 $P(广告|正常邮件)=5/240=1/48$，“广告”出现在垃圾邮件里面出现的概率 $P(广告|垃圾邮件)=4/120=1/30$，所以广告这个词更倾向于在垃圾邮件里面出现，也比较符合我们的常理。\n",
    " \n",
    " 通过上面的方式，我们计算出了“购买物品，不是广告”这句话中的每个单词的条件概率，在现实的我们在构建朴素贝叶斯模型的时候，我们需要对词库（词典）里面的每一个单词计算条件概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先验概率\n",
    "\n",
    "另外我们在构建朴素贝叶斯模型的时候，还需要一个信息Prior Information(先验)，什么是先验呢？在如图所示的场景下，先验就是正常邮件在整个邮件里面的占比是多少，垃圾邮件在我们整个邮件数量里面的占比是多少。\n",
    "\n",
    "  ![](./img/9.jpg)\n",
    "  \n",
    "  先验顾名思义就是我们已知的信息，例如我们的例子里面包含了 24 个正常邮件，12 个垃圾邮件，我们通过简单的统计就可以计算出正常邮件的占比是多少 $P(正常邮件)=2/3$，$P(垃圾邮件)=1/3$，所以这两个概率就是我们的先验证知识，在朴素贝叶斯模型里面会用到先验概率，我们需要把这两个概率先统计出来。\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯定理\n",
    "\n",
    "因为朴素贝叶斯我们在计算的时候会用到贝叶斯定理，在前面的内容我们能讲到的更多的是如何计算每个单词在不同的分类中的概率，这个概率是整个朴素贝叶斯的一个基础，但是我们在去识别一个邮件是不是属于正常邮件还是垃圾邮件的时候，我们需要用到贝叶斯定理。\n",
    "\n",
    "贝叶斯定理就是假设我有两个随机变量 $X $ 和 $Y$，$P(X|Y)$ 这样的一个条件概率可以写成\n",
    "\n",
    "$$P(X|Y) = P(Y|X) * P(X)/P(Y)$$\n",
    "\n",
    "这个公式就是著名的贝叶斯定理，$P(X|Y)$ 这个条件概率是给定 $Y$ 然后预测 $X$，$P(Y|X)$ 相反，$X$ 成了已知的条件，$Y$ 是需要预测的。\n",
    "\n",
    "那怎么得来的呢？\n",
    "\n",
    "$P(X,Y)$ 是 $X$ 和 $Y$ 的联合概率，联合概率有两种不同的写法\n",
    "\n",
    "$$P(X,Y)=P(X)*P(Y|X)$$\n",
    "$$P(X,Y) = P(Y)*P(X|Y)$$\n",
    "\n",
    "之后就可以得到 $P(X)*P(Y|X) = P(Y)*P(X|Y)$，然后通过简单的变形就得到 $P(X|Y) =P(Y|X)*P(X)/P(Y)$，也就得到了贝叶斯定理，贝叶斯定理就是这么推导的。\n",
    "\n",
    "![](./img/10.jpg)\n",
    "\n",
    "### 条件独立性\n",
    " \n",
    "接下来我们简单介绍一下 Conditional Independence(条件独立)这样的一个定理，条件独立的概念应用也是非常广泛的。\n",
    "\n",
    "在条件概率下面我们可以有 $P(X,Y|Z)$，有 $X$ 和 $ Y$ 给定 $Z$，意思就是在给定 $Z$ 的时候，要计算出 $P(X,Y|Z)$ 这样一个条件概率，然后假设 $X$ 和 $Y$ 是条件独立于 $Z$ 的，这个时候我们可以把条件概率写成上图中的形式。\n",
    "\n",
    "$$P(X,Y|Z) = P(X|Z) * P(Y|Z)$$\n",
    "\n",
    "这就是经典的条件独立的性质，X和Y是条件独立于变量Z的，现在我们只需要记住这个性质就好了，我们也会在朴素贝叶斯里面用到这个性质。\n",
    "\n",
    "![](./img/11.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测邮件\n",
    "\n",
    "接下来我们看一下对于一个新的邮件如何去预测它是正常邮件还是垃圾邮件，为了预测我们需要计算出 $P(正常邮件|邮件内容)$ 就是给定邮件内容计算出它属于正常邮件的概率是多少，同时也需要计算出它属于垃圾邮件的概率是多少 $P(垃圾邮件|邮件内容)$，最后计算完之后比较两者之间的大小，$P(正常邮件|邮件内容) >= P(垃圾邮件|邮件内容)$，就是正常邮件，否则分类成垃圾邮件。\n",
    "\n",
    "那 $P(正常邮件|邮件内容)$ 如何计算呢？通过贝叶斯定理我们可以得出\n",
    "\n",
    "$$ P(正常|内容) = P(内容|正常)*P(正常)/P(内容)$$ \n",
    "\n",
    "P(垃圾邮件|邮件内容)通过贝叶斯定理可以得到\n",
    "\n",
    "$$ P(垃圾|内容)=P(内容|垃圾)*P(垃圾)/P(内容)$$ \n",
    "\n",
    "然后可以看出两者的分母是一样的，我们就可以只计算分子 $P(内容|正常)*P(正常)和P(内容|垃圾)*P(垃圾)$，然后比大小。\n",
    "\n",
    "接下来的关键就是如何计算分子的每一项，$P(正常)$ 和 $P(垃圾)$ 就是我们前面讲到的先验概率，并且概率也已经计算出来了，P(正常)=2/3，P(垃圾)=1/3。\n",
    "\n",
    "然后计算 $P(内容|正常)$ 和 $P(内容|垃圾$)，我们可以把“内容”做一个展开，然后得到\n",
    "\n",
    "$$ P(内容|正常)= P(购买，物品，不是，广告|正常)$$ \n",
    "\n",
    "展开的前提是假定我们已经使用了分词工具，通过分词工具我们把“购买物品，不是广告”这一句话分成了四个单词。\n",
    "\n",
    "接着我们再通过前面讲到的条件独立的性质再把 $P(购买，物品，不是，广告|正常)$ 做一个简化，我们将“购买”，“物品”，“不是”，“广告”每一个都看成是一个变量，这个变量是独立于“正常”这个变量的，简化完之后就变成\n",
    "\n",
    "$$ P(购买，物品，不是，广告|正常)=P(购买|正常)* P(物品|正常)* P(不是|正常)* P(广告|正常)$$ \n",
    "\n",
    "![](./img/12.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 3 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重点来了-数学与代码的对应\n",
    "\n",
    "- 先验概率\n",
    "\n",
    "垃圾短信 / 总短信的条数\n",
    "\n",
    "正常短信 / 总短信的条数 \n",
    "\n",
    "垃圾短信 / 总短信的条数 = 1 - 正常短信 / 总短信的条数 \n",
    "\n",
    "垃圾短信的标签是 1，通过求和，就可以获得个数\n",
    "\n",
    "对应代码的位置：\n",
    "```\n",
    "[\n",
    "[1 0 0 1 1 1],\n",
    "[0 1 0 1 0 0],\n",
    ".....\n",
    "[0 0 0 0 0 0]\n",
    "]\n",
    "A (8000, 6)\n",
    "\n",
    "A[0] [0]\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "def trainNB(trainMatrix, trainCategory):\n",
    "\t# trainMatrix是二维 array 类型，为词向量矩阵\n",
    "\t# trainCategory 是一维 array 类型\n",
    "\tnumTrainDocs = len(trainMatrix) # 用这里：总短信条数-ie:8000\n",
    "\tprint('numTrainDocs: ', numTrainDocs)\n",
    "\tnumWords = len(trainMatrix[0]) # 总词数 6\n",
    "\tprint('numWords : ', numWords)\n",
    "\t# 垃圾短信的概率\n",
    "\tpAbusive = sum(trainCategory) / float(numTrainDocs) # 用这里： 正常短信：1-pAbusive \n",
    "```\n",
    "- 一个词在垃圾短信中出现的概率\n",
    "\n",
    "- - 这个词在垃圾短信中出现的总次数？\n",
    "\n",
    "- - 垃圾短信总共有多少个词？\n",
    "\n",
    "- 一个词在正常短信中出现的概率\n",
    "\n",
    "- 关于概率\n",
    "\n",
    "我们面临的第一个挑战是计算问题。正如我们上面所说的，我们将乘以很多概率（每个词一个概率），这些概率往往很小。\n",
    "\n",
    "当在内部表示非常大或非常小的数字时，现代计算机体系结构只能处理一定的精度。如果我们不断地乘以小数，我们可能会遇到所谓的“算术下溢”问题，最终会将我们的数字变成 0。\n",
    "\n",
    "处理此问题的一个众所周知的技巧是使用指数函数和对数的组合。这里采用对数。\n",
    "\n",
    "$$ log(a*b) = log(a) + log(b) $$\n",
    "\n",
    "\n",
    "第二个问题植根于我们试图进行的数学计算。让我们通过研究一个我们很可能会遇到的案例来理解这个问题。\n",
    "\n",
    "如果某个词只出现垃圾短信中呢？那在正常短信中概率为0，若跟其他概率相乘，也为0。\n",
    "\n",
    "为了解决这个问题，我们可以引入一个因素 k这是一个我们可以调整的参数（大多数时候它被设置为1个）。\n",
    "\n",
    "$$ P(词 W | 垃圾短信) = \\frac{k（大多数设置为1） + 垃圾短信中 W 的总数}{（2\\times k）+ 垃圾邮件的总词数}$$\n",
    "\n",
    "对应代码：\n",
    "\n",
    "```python\n",
    "def trainNB(trainMatrix, trainCategory):\n",
    "\t# trainMatrix是二维 array 类型，为词向量矩阵\n",
    "\t# trainCategory 是一维 array 类型\n",
    "\tnumTrainDocs = len(trainMatrix) # 总短信条数\n",
    "\tprint('numTrainDocs: ', numTrainDocs)\n",
    "\tnumWords = len(trainMatrix[0]) # 总词数\n",
    "\tprint('numWords : ', numWords)\n",
    "\t# spam 垃圾短信的概率\n",
    "\tpAbusive = sum(trainCategory) / float(numTrainDocs) # 正常短信：1-pAbusive \n",
    "    \n",
    "\t# 这里开始：短信中某个词出现的概率\n",
    "\tp0Num = ones(numWords) # array数组，初始化为1,例如 array([1., 1., 1.])\n",
    "\tprint('p0Num.size', p0Num.shape)\n",
    "\tp1Num = ones(numWords) #防止多个概率相乘为0\n",
    "\tprint('p1Num.size', p1Num.shape)\n",
    "\tp0Denom = 2.0 #浮点型数据\n",
    "\tp1Denom = 2.0 #垃圾短信总词数\n",
    "\tfor i in range(numTrainDocs):\n",
    "\t\tif trainCategory[i] == 1: # 垃圾短信\n",
    "\t\t\tp1Num += trainMatrix[i] # 添加此条垃圾短信的每个词的数量，向量\n",
    "\t\t\tp1Denom += sum(trainMatrix[i]) # 加上此条垃圾短信的词数，整数\n",
    "\t\telse:\n",
    "\t\t\tp0Num += trainMatrix[i]\n",
    "\t\t\tp0Denom += sum(trainMatrix[i])\n",
    "\tp1Vec = log(p1Num/p1Denom) #array数组除浮点数，得array数组\n",
    "\tp0Vec = log(p0Num/p0Denom) #取log，之后相加即可\n",
    "\t#每个词在普通短信中出现的概率，p(w|c0)\n",
    "\treturn p0Vec, p1Vec,pAbusive\n",
    "\t# 两个向量，一个垃圾短信概率\n",
    "    # 这里结束：短信中某个词出现的概率\n",
    "```\n",
    "\n",
    "**预测就是：传入短信内容，进行分词，计算每个分词在垃圾和正常短信中出现的概率，如在垃圾出现的概率大于正常，就是垃圾短信。**\n",
    "\n",
    "对应代码是：\n",
    "\n",
    "```python\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "\t#判断vec2Classify向量的分类\n",
    "\tp1 = sum(vec2Classify * p1Vec) + log(pClass1)\n",
    "\t# sum(vec2Classify * p1Vec)为log(p(w|c))\n",
    "\n",
    "\t# p1为log( p(w|c)*p(c1) )\n",
    "\tp0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "\t\n",
    "\tif p1 > p0:\n",
    "\t\treturn 1 #为垃圾短信\n",
    "\telse:\n",
    "\t\treturn 0 #为正常短信\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原始代码之模型-在此 notebook 上运行-还需调整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "'''\n",
    "trainNB(trainMatrix, trainCategory):\n",
    "训练出模型\n",
    "\n",
    "trainMatrix 是二维array类型，为词向量矩阵,每一行为词向量\n",
    "array((1,0,1,0,1,0),(0,1,1,1,0,1),(0,0,1,1,1,0))\n",
    "\n",
    "trainCategory 是一维array类型，标记每个词向量的类型\n",
    "array(1,0,0,0,1,0,1)\n",
    "\n",
    "p0Vec,p1Vec:array型一维数组，与词向量等长，表示在0/1型短\n",
    "信中各个词出现的概率，即 P(w|c)\n",
    "\n",
    "pAbusive:浮点数，表示垃圾短信的概率，即 P(c1),标签1为垃圾短信，标签0为正常短信\n",
    "\n",
    "'''\n",
    "def trainNB(trainMatrix, trainCategory):\n",
    "\t# trainMatrix是二维 array 类型，为词向量矩阵\n",
    "\t# trainCategory 是一维 array 类型\n",
    "\tnumTrainDocs = len(trainMatrix) # 总短信条数\n",
    "\tprint('numTrainDocs: ', numTrainDocs)\n",
    "\tnumWords = len(trainMatrix[0]) # 总词数\n",
    "\tprint('numWords : ', numWords)\n",
    "\t# 垃圾短信的概率\n",
    "\tpAbusive = sum(trainCategory) / float(numTrainDocs) # 正常短信：1-pAbusive \n",
    "\t# spam（垃圾邮件）短信的概率\n",
    "\tp0Num = ones(numWords) #array数组，初始化为1,例如 array([1., 1., 1.])\n",
    "\tprint('p0Num.size', p0Num.shape)\n",
    "\tp1Num = ones(numWords) #防止多个概率相乘为0\n",
    "\tprint('p1Num.size', p1Num.shape)\n",
    "\tp0Denom = 2.0 #浮点型数据\n",
    "\tp1Denom = 2.0 #垃圾短信总词数\n",
    "\tfor i in range(numTrainDocs):\n",
    "\t\tif trainCategory[i] == 1: # 垃圾短信\n",
    "\t\t\tp1Num += trainMatrix[i] # 添加此条垃圾短信的每个词的数量，向量\n",
    "\t\t\tp1Denom += sum(trainMatrix[i]) # 加上此条垃圾短信的词数，整数\n",
    "\t\telse:\n",
    "\t\t\tp0Num += trainMatrix[i]\n",
    "\t\t\tp0Denom += sum(trainMatrix[i])\n",
    "\tp1Vec = log(p1Num/p1Denom) #array数组除浮点数，得array数组\n",
    "\tp0Vec = log(p0Num/p0Denom) #取log，之后相加即可\n",
    "\t#每个词在普通短信中出现的概率，p(w|c0)\n",
    "\treturn p0Vec,p1Vec,pAbusive\n",
    "\t# 两个向量，一个垃圾短信概率\n",
    "\n",
    "'''\n",
    "\n",
    "classifyNB(vec2Classify, p0Vec, p1Vec, pClass):\n",
    "判断vec2Classify这个词向量所属类型\n",
    "\n",
    "vec2Classify是词向量，(1,0,0,1,0,1)\n",
    "p0Vec,p1Vec,PClass 为trainNB的三个输出\n",
    "\n",
    "'''\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "\t#判断vec2Classify向量的分类\n",
    "\tp1 = sum(vec2Classify * p1Vec) + log(pClass1)\n",
    "\t# sum(vec2Classify * p1Vec)为log(p(w|c))\n",
    "\n",
    "\t# p1为log( p(w|c)*p(c1) )\n",
    "\tp0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "\t\n",
    "\tif p1 > p0:\n",
    "\t\treturn 1 #为垃圾短信\n",
    "\telse:\n",
    "\t\treturn 0 #为正常短信\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 什么是 Flask Python\n",
    "\n",
    "Flask 是一个 Web 框架，它是一个 Python 模块，可让读者轻松开发 Web 应用程序。它有一个小而易于扩展的核心：它是一个不包含 ORM（对象关系管理器）或类似功能的微框架。它确实有很多很酷的功能，比如 url 路由、模板引擎。它是一个 WSGI 网络应用程序框架。\n",
    "\n",
    "什么是 Flask \n",
    "Flask 是一个用 Python 编写的 Web 应用程序框架。它由 Armin Ronacher 开发，他领导了一个名为 Poocco 的国际 Python 爱好者团队。Flask 基于 Werkzeg WSGI 工具包和 Jinja2 模板引擎。两者都是 Pocco 项目。\n",
    "\n",
    "WSGI\n",
    "网络服务器网关接口（Web Server Gateway Interface，WSGI）已经被用作Python网络应用开发的标准。WSGI 是 Web 服务器和 Web 应用程序之间通用接口的规范。\n",
    "\n",
    "1.  `from flask import Flask`，首先用 `import` 导入了 `Flask` 类。 该类的实例将会成为 WSGI 应用。\n",
    "\n",
    "2.  `app = Flask(__name__)` 接着创建一个该类的实例。第一个参数是应用模块或者包的名称。如果你使用 一个单一模块（就像本例），那么应当使用 name ，因为名称会根据这个 模块是按应用方式使用还是作为一个模块导入而发生变化（可能是 `‘main’` ， 也可能是实际导入的名称）。这个参数是必需的，这样 Flask 才能知道在哪里可以 找到模板和静态文件等东西。\n",
    "\n",
    "3.  `@app.route('/')` 然后我们使用 `route()` 装饰器来告诉 Flask 触发函数的 URL：http://127.0.0.1:5000/ 。\n",
    "\n",
    "补充：\n",
    "\n",
    "[1]. 前端基础语言HTML、CSS 和 JavaScript 学习指南: https://baijiahao.baidu.com/s?id=1728005039778333832&wfr=spider&for=pc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用到的代码\n",
    "\n",
    "application.py\n",
    "\n",
    "handWriting_bayes.LoadData\n",
    "\n",
    "handWriting_bayes.bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
